{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "np.random.seed(0)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Download MNIST dataset\n",
    "emnist_train = torchvision.datasets.EMNIST('./dataset/', download=True, train=True, split='digits')\n",
    "emnist_eval = torchvision.datasets.EMNIST('./dataset', download=True, train=False, split='digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data in a matrix of form [batch, dim]\n",
    "# Create list of correct_labels for train and eval sets\n",
    "\n",
    "dim = 28*28\n",
    "\n",
    "n_images_train = len(emnist_train)\n",
    "n_images_eval = len(emnist_eval)\n",
    "\n",
    "train_data = np.empty([n_images_train, dim])\n",
    "train_correct_labels = []\n",
    "\n",
    "eval_data = np.empty([n_images_eval, dim])\n",
    "eval_correct_labels = []\n",
    "\n",
    "for i in range(n_images_train):\n",
    "    train_data[i] = np.array(emnist_train[i][0]).reshape(1, dim)\n",
    "    train_correct_labels.append(emnist_train[i][1])\n",
    "\n",
    "for i in range(n_images_eval):\n",
    "    eval_data[i] = np.array(emnist_eval[i][0]).reshape(1, dim)\n",
    "    eval_correct_labels.append(emnist_eval[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.VectorSpace import VectorSpace\n",
    "from src.VectorSet import VectorSet\n",
    "\n",
    "def cossine_similarity(vector:np.ndarray, subspace:VectorSpace) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns S = \\sum_{i=0}^{r-1} \\frac{(x,\\phi_i)^2}{\\|x\\|\\|\\phi_i\\|}\n",
    "    \"\"\"\n",
    "    if vector.ndim > 2:\n",
    "        raise(AssertionError(\"Cannot input tensor of ndim > 2\"))\n",
    "    if vector.ndim == 1:\n",
    "        vector = vector[np.newaxis, :]\n",
    "    if vector.shape[1] != subspace.dim:\n",
    "        raise(AssertionError(\"Vector dimension must be the same as VectorSpace dimension\"))       \n",
    "\n",
    "    vector = vector.astype(subspace.dtype)\n",
    "\n",
    "    S = np.sum(\n",
    "            np.divide(\n",
    "                np.matmul(vector, subspace.A.transpose())**2,\n",
    "                np.matmul(\n",
    "                    np.sqrt(\n",
    "                        np.diag(\n",
    "                            np.matmul(vector, vector.transpose()\n",
    "                            )\n",
    "                        )\n",
    "                    )[np.newaxis, :].transpose(),\n",
    "                    np.sqrt(\n",
    "                        np.diag(\n",
    "                            np.matmul(subspace.A, subspace.A.transpose())\n",
    "                        )\n",
    "                    )[np.newaxis, :]\n",
    "                )\n",
    "            ), axis=1\n",
    "        )\n",
    "    return S\n",
    "\n",
    "def scaled_cossine_similarity(vector:np.ndarray, subspace:VectorSpace) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns S = \\sum_{i=0}^{r-1} \\frac{\\sigma_i}{\\sum{\\sigma}} \\frac{(x,\\phi_i)^2}{\\|x\\|\\|\\phi_i\\|}\n",
    "    \"\"\"\n",
    "    if vector.ndim > 2:\n",
    "        raise(AssertionError(\"Cannot input tensor of ndim > 2\"))\n",
    "    if vector.ndim == 1:\n",
    "        vector = vector[np.newaxis, :]\n",
    "    if vector.shape[1] != subspace.dim:\n",
    "        raise(AssertionError(\"Vector dimension must be the same as VectorSpace dimension\"))       \n",
    "\n",
    "    vector = vector.astype(subspace.dtype)\n",
    "\n",
    "    S = np.inner(\n",
    "            np.divide(\n",
    "                np.matmul(vector, subspace.A.transpose())**2,\n",
    "                np.matmul(\n",
    "                    np.sqrt(\n",
    "                        np.diag(\n",
    "                            np.matmul(vector, vector.transpose()\n",
    "                            )\n",
    "                        )\n",
    "                    )[np.newaxis, :].transpose(),\n",
    "                    np.sqrt(\n",
    "                        np.diag(\n",
    "                            np.matmul(subspace.A, subspace.A.transpose())\n",
    "                        )\n",
    "                    )[np.newaxis, :]\n",
    "                )\n",
    "            ),\n",
    "            (np.array(subspace.singular_values, dtype=subspace.dtype) / np.max(subspace.singular_values))\n",
    "        )\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(a:np.ndarray, l:list, n:int) -> Tuple[np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Shuffles the elements of two lists up to a specified index.\n",
    "\n",
    "    Args:\n",
    "        a (np.ndarray): An array-like object containing elements of type np.ndarray.\n",
    "        l (list): A list containing elements of any type.\n",
    "        n (int): The index up to which the lists should be shuffled.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, list]: A tuple containing two shuffled lists, where the elements\n",
    "        up to index n have been shuffled.\n",
    "    \"\"\"\n",
    "    combined = list(zip(a, l))\n",
    "    np.random.shuffle(combined)\n",
    "    shuffled_list1, shuffled_list2 = zip(*combined)\n",
    "    return np.array(shuffled_list1[:n]), list(shuffled_list2[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1626b80f9fc42bfbbc948d9e6d7084b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity std: 0.013344006145082498\n",
      "scaled_cosine_similarity std: 0.017792203348658085\n",
      "sd <- 0.015459624833740322\n",
      "sd.ratio <- 1.3333479582677519\n"
     ]
    }
   ],
   "source": [
    "# pilot study\n",
    "\n",
    "n_train = int(len(train_data) / 100)\n",
    "n_eval = int(len(eval_data) / 100)\n",
    "repetitions = 10\n",
    "min_energy = 0.5\n",
    "\n",
    "cs_std_list = []\n",
    "scs_std_list = []\n",
    "\n",
    "for _ in tqdm(range(repetitions)):\n",
    "    train_data_bootstrap, train_correct_labels_bootstrap = shuffle(train_data, train_correct_labels, n_train)\n",
    "    eval_data_bootstrap, eval_correct_labels_bootstrap = shuffle(eval_data, eval_correct_labels, n_eval)\n",
    "\n",
    "    # Create a VectorSet for all VectorSpaces\n",
    "    set = VectorSet(dim=dim)\n",
    "    set.populate(train_data_bootstrap, train_correct_labels_bootstrap)\n",
    "\n",
    "    # Generate Subspaces using pca (svd) and maintain the N biggest eigenvectors, energy(N) > energy(min_energy)\n",
    "    subset = set.pca(min_energy=min_energy)\n",
    "\n",
    "    # Create a list of max likelihood using the traditional cossine similarity and the scaled cossine similarity\n",
    "    max_likelihood_cs = [None]*eval_data_bootstrap.shape[0]\n",
    "    cs_list = [0]*eval_data_bootstrap.shape[0]\n",
    "    \n",
    "    max_likelihood_scs = [None]*eval_data_bootstrap.shape[0]\n",
    "    scs_list = [0]*eval_data_bootstrap.shape[0]\n",
    "\n",
    "    # Classify the eval_data_bootstrap\n",
    "    for subspace in subset:\n",
    "        cs = cossine_similarity(eval_data_bootstrap, subspace)\n",
    "        scs = scaled_cossine_similarity(eval_data_bootstrap, subspace)\n",
    "        for i in range(len(cs)):\n",
    "            if cs[i] > cs_list[i]: cs_list[i] = cs[i]; max_likelihood_cs[i] = subspace.label\n",
    "            if scs[i] > scs_list[i]: scs_list[i] = scs[i]; max_likelihood_scs[i] = subspace.label\n",
    "\n",
    "    correct_class_cs = []\n",
    "    correct_class_scs = []\n",
    "    for l1, l2 in zip(max_likelihood_cs, eval_correct_labels_bootstrap):\n",
    "        correct_class_cs.append(l1 == l2)\n",
    "    for l1, l2 in zip(max_likelihood_scs, eval_correct_labels_bootstrap):\n",
    "        correct_class_scs.append(l1 == l2)\n",
    "\n",
    "    prediction_ratio_cs = correct_class_cs.count(True) / len(correct_class_cs)\n",
    "    prediction_ratio_scs = correct_class_scs.count(True) / len(correct_class_scs)\n",
    "    cs_std_list.append(prediction_ratio_cs)\n",
    "    scs_std_list.append(prediction_ratio_scs)\n",
    "\n",
    "print(f\"cosine_similarity std: {np.std(cs_std_list)}\")\n",
    "print(f\"scaled_cosine_similarity std: {np.std(scs_std_list)}\")\n",
    "print(f\"sd <- {np.std(np.subtract(scs_std_list,cs_std_list))}\")\n",
    "print(f\"sd.ratio <- {np.std(scs_std_list)/np.std(cs_std_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6348745723b4247b235e98f0681db7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10, cosine_similarity, 0.805\n",
      "0.10, scaled_cosine_similarity, 0.805\n",
      "0.10, cosine_similarity, 0.7675\n",
      "0.10, scaled_cosine_similarity, 0.7675\n",
      "0.10, cosine_similarity, 0.8025\n",
      "0.10, scaled_cosine_similarity, 0.8025\n",
      "0.10, cosine_similarity, 0.835\n",
      "0.10, scaled_cosine_similarity, 0.835\n",
      "0.10, cosine_similarity, 0.8025\n",
      "0.10, scaled_cosine_similarity, 0.8025\n",
      "0.10, cosine_similarity, 0.8075\n",
      "0.10, scaled_cosine_similarity, 0.8075\n",
      "0.10, cosine_similarity, 0.855\n",
      "0.10, scaled_cosine_similarity, 0.83\n",
      "0.10, cosine_similarity, 0.7725\n",
      "0.10, scaled_cosine_similarity, 0.7725\n",
      "0.10, cosine_similarity, 0.805\n",
      "0.10, scaled_cosine_similarity, 0.805\n",
      "0.10, cosine_similarity, 0.785\n",
      "0.10, scaled_cosine_similarity, 0.785\n",
      "0.20, cosine_similarity, 0.91\n",
      "0.20, scaled_cosine_similarity, 0.8775\n",
      "0.20, cosine_similarity, 0.91\n",
      "0.20, scaled_cosine_similarity, 0.86\n",
      "0.20, cosine_similarity, 0.9025\n",
      "0.20, scaled_cosine_similarity, 0.8775\n",
      "0.20, cosine_similarity, 0.8975\n",
      "0.20, scaled_cosine_similarity, 0.8675\n",
      "0.20, cosine_similarity, 0.89\n",
      "0.20, scaled_cosine_similarity, 0.8375\n",
      "0.20, cosine_similarity, 0.8775\n",
      "0.20, scaled_cosine_similarity, 0.85\n",
      "0.20, cosine_similarity, 0.89\n",
      "0.20, scaled_cosine_similarity, 0.87\n",
      "0.20, cosine_similarity, 0.915\n",
      "0.20, scaled_cosine_similarity, 0.88\n",
      "0.20, cosine_similarity, 0.8825\n",
      "0.20, scaled_cosine_similarity, 0.845\n",
      "0.20, cosine_similarity, 0.8425\n",
      "0.20, scaled_cosine_similarity, 0.8125\n",
      "0.30, cosine_similarity, 0.9525\n",
      "0.30, scaled_cosine_similarity, 0.89\n",
      "0.30, cosine_similarity, 0.935\n",
      "0.30, scaled_cosine_similarity, 0.8675\n",
      "0.30, cosine_similarity, 0.9275\n",
      "0.30, scaled_cosine_similarity, 0.8725\n",
      "0.30, cosine_similarity, 0.9225\n",
      "0.30, scaled_cosine_similarity, 0.86\n",
      "0.30, cosine_similarity, 0.91\n",
      "0.30, scaled_cosine_similarity, 0.855\n",
      "0.30, cosine_similarity, 0.9475\n",
      "0.30, scaled_cosine_similarity, 0.8775\n",
      "0.30, cosine_similarity, 0.9475\n",
      "0.30, scaled_cosine_similarity, 0.8775\n",
      "0.30, cosine_similarity, 0.925\n",
      "0.30, scaled_cosine_similarity, 0.89\n",
      "0.30, cosine_similarity, 0.9375\n",
      "0.30, scaled_cosine_similarity, 0.8875\n",
      "0.30, cosine_similarity, 0.93\n",
      "0.30, scaled_cosine_similarity, 0.875\n",
      "0.40, cosine_similarity, 0.955\n",
      "0.40, scaled_cosine_similarity, 0.8875\n",
      "0.40, cosine_similarity, 0.93\n",
      "0.40, scaled_cosine_similarity, 0.8725\n",
      "0.40, cosine_similarity, 0.9275\n",
      "0.40, scaled_cosine_similarity, 0.88\n",
      "0.40, cosine_similarity, 0.945\n",
      "0.40, scaled_cosine_similarity, 0.8825\n",
      "0.40, cosine_similarity, 0.9475\n",
      "0.40, scaled_cosine_similarity, 0.875\n",
      "0.40, cosine_similarity, 0.93\n",
      "0.40, scaled_cosine_similarity, 0.8225\n",
      "0.40, cosine_similarity, 0.915\n",
      "0.40, scaled_cosine_similarity, 0.865\n",
      "0.40, cosine_similarity, 0.9575\n",
      "0.40, scaled_cosine_similarity, 0.9025\n",
      "0.40, cosine_similarity, 0.94\n",
      "0.40, scaled_cosine_similarity, 0.9025\n",
      "0.40, cosine_similarity, 0.94\n",
      "0.40, scaled_cosine_similarity, 0.87\n",
      "0.50, cosine_similarity, 0.96\n",
      "0.50, scaled_cosine_similarity, 0.885\n",
      "0.50, cosine_similarity, 0.94\n",
      "0.50, scaled_cosine_similarity, 0.8625\n",
      "0.50, cosine_similarity, 0.9525\n",
      "0.50, scaled_cosine_similarity, 0.895\n",
      "0.50, cosine_similarity, 0.945\n",
      "0.50, scaled_cosine_similarity, 0.9\n",
      "0.50, cosine_similarity, 0.9325\n",
      "0.50, scaled_cosine_similarity, 0.885\n",
      "0.50, cosine_similarity, 0.9525\n",
      "0.50, scaled_cosine_similarity, 0.8825\n",
      "0.50, cosine_similarity, 0.955\n",
      "0.50, scaled_cosine_similarity, 0.8925\n",
      "0.50, cosine_similarity, 0.9575\n",
      "0.50, scaled_cosine_similarity, 0.905\n",
      "0.50, cosine_similarity, 0.95\n",
      "0.50, scaled_cosine_similarity, 0.875\n",
      "0.50, cosine_similarity, 0.955\n",
      "0.50, scaled_cosine_similarity, 0.885\n",
      "0.60, cosine_similarity, 0.93\n",
      "0.60, scaled_cosine_similarity, 0.8475\n",
      "0.60, cosine_similarity, 0.955\n",
      "0.60, scaled_cosine_similarity, 0.8925\n",
      "0.60, cosine_similarity, 0.9325\n",
      "0.60, scaled_cosine_similarity, 0.8625\n",
      "0.60, cosine_similarity, 0.94\n",
      "0.60, scaled_cosine_similarity, 0.9025\n",
      "0.60, cosine_similarity, 0.91\n",
      "0.60, scaled_cosine_similarity, 0.8675\n",
      "0.60, cosine_similarity, 0.9325\n",
      "0.60, scaled_cosine_similarity, 0.88\n",
      "0.60, cosine_similarity, 0.95\n",
      "0.60, scaled_cosine_similarity, 0.86\n",
      "0.60, cosine_similarity, 0.945\n",
      "0.60, scaled_cosine_similarity, 0.9\n",
      "0.60, cosine_similarity, 0.94\n",
      "0.60, scaled_cosine_similarity, 0.8825\n",
      "0.60, cosine_similarity, 0.9525\n",
      "0.60, scaled_cosine_similarity, 0.8775\n",
      "0.70, cosine_similarity, 0.935\n",
      "0.70, scaled_cosine_similarity, 0.88\n",
      "0.70, cosine_similarity, 0.94\n",
      "0.70, scaled_cosine_similarity, 0.875\n",
      "0.70, cosine_similarity, 0.9475\n",
      "0.70, scaled_cosine_similarity, 0.8725\n",
      "0.70, cosine_similarity, 0.92\n",
      "0.70, scaled_cosine_similarity, 0.8675\n",
      "0.70, cosine_similarity, 0.94\n",
      "0.70, scaled_cosine_similarity, 0.875\n",
      "0.70, cosine_similarity, 0.925\n",
      "0.70, scaled_cosine_similarity, 0.8675\n",
      "0.70, cosine_similarity, 0.925\n",
      "0.70, scaled_cosine_similarity, 0.8675\n",
      "0.70, cosine_similarity, 0.935\n",
      "0.70, scaled_cosine_similarity, 0.9\n",
      "0.70, cosine_similarity, 0.9475\n",
      "0.70, scaled_cosine_similarity, 0.88\n",
      "0.70, cosine_similarity, 0.9375\n",
      "0.70, scaled_cosine_similarity, 0.855\n",
      "0.80, cosine_similarity, 0.9375\n",
      "0.80, scaled_cosine_similarity, 0.89\n",
      "0.80, cosine_similarity, 0.9475\n",
      "0.80, scaled_cosine_similarity, 0.875\n",
      "0.80, cosine_similarity, 0.92\n",
      "0.80, scaled_cosine_similarity, 0.8975\n",
      "0.80, cosine_similarity, 0.935\n",
      "0.80, scaled_cosine_similarity, 0.9075\n",
      "0.80, cosine_similarity, 0.915\n",
      "0.80, scaled_cosine_similarity, 0.8675\n",
      "0.80, cosine_similarity, 0.9375\n",
      "0.80, scaled_cosine_similarity, 0.8825\n",
      "0.80, cosine_similarity, 0.9425\n",
      "0.80, scaled_cosine_similarity, 0.905\n",
      "0.80, cosine_similarity, 0.935\n",
      "0.80, scaled_cosine_similarity, 0.855\n",
      "0.80, cosine_similarity, 0.9275\n",
      "0.80, scaled_cosine_similarity, 0.8825\n",
      "0.80, cosine_similarity, 0.93\n",
      "0.80, scaled_cosine_similarity, 0.8875\n",
      "0.90, cosine_similarity, 0.91\n",
      "0.90, scaled_cosine_similarity, 0.8475\n",
      "0.90, cosine_similarity, 0.9025\n",
      "0.90, scaled_cosine_similarity, 0.88\n",
      "0.90, cosine_similarity, 0.9025\n",
      "0.90, scaled_cosine_similarity, 0.8425\n",
      "0.90, cosine_similarity, 0.92\n",
      "0.90, scaled_cosine_similarity, 0.865\n",
      "0.90, cosine_similarity, 0.92\n",
      "0.90, scaled_cosine_similarity, 0.8775\n",
      "0.90, cosine_similarity, 0.9025\n",
      "0.90, scaled_cosine_similarity, 0.89\n",
      "0.90, cosine_similarity, 0.91\n",
      "0.90, scaled_cosine_similarity, 0.8825\n",
      "0.90, cosine_similarity, 0.91\n",
      "0.90, scaled_cosine_similarity, 0.845\n",
      "0.90, cosine_similarity, 0.95\n",
      "0.90, scaled_cosine_similarity, 0.8925\n",
      "0.90, cosine_similarity, 0.915\n",
      "0.90, scaled_cosine_similarity, 0.87\n",
      "1.00, cosine_similarity, 0.8725\n",
      "1.00, scaled_cosine_similarity, 0.8675\n",
      "1.00, cosine_similarity, 0.8975\n",
      "1.00, scaled_cosine_similarity, 0.895\n",
      "1.00, cosine_similarity, 0.8775\n",
      "1.00, scaled_cosine_similarity, 0.8575\n",
      "1.00, cosine_similarity, 0.86\n",
      "1.00, scaled_cosine_similarity, 0.865\n",
      "1.00, cosine_similarity, 0.87\n",
      "1.00, scaled_cosine_similarity, 0.88\n",
      "1.00, cosine_similarity, 0.8375\n",
      "1.00, scaled_cosine_similarity, 0.8725\n",
      "1.00, cosine_similarity, 0.865\n",
      "1.00, scaled_cosine_similarity, 0.8775\n",
      "1.00, cosine_similarity, 0.9025\n",
      "1.00, scaled_cosine_similarity, 0.855\n",
      "1.00, cosine_similarity, 0.8725\n",
      "1.00, scaled_cosine_similarity, 0.8575\n",
      "1.00, cosine_similarity, 0.885\n",
      "1.00, scaled_cosine_similarity, 0.8925\n"
     ]
    }
   ],
   "source": [
    "# List of min energy for parameter tunning\n",
    "min_energy_list = np.linspace(0.1, 1, 10)\n",
    "n_train = int(len(train_data) / 100)\n",
    "n_eval = int(len(eval_data) / 100)\n",
    "repetitions = 10\n",
    "\n",
    "for min_energy in tqdm(min_energy_list):\n",
    "    for _ in range(repetitions):\n",
    "        train_data_bootstrap, train_correct_labels_bootstrap = shuffle(train_data, train_correct_labels, n_train)\n",
    "        eval_data_bootstrap, eval_correct_labels_bootstrap = shuffle(eval_data, eval_correct_labels, n_eval)\n",
    "\n",
    "        # Create a VectorSet for all VectorSpaces\n",
    "        set = VectorSet(dim=dim)\n",
    "        set.populate(train_data_bootstrap, train_correct_labels_bootstrap)\n",
    "\n",
    "        # Generate Subspaces using pca (svd) and maintain the N biggest eigenvectors, energy(N) > energy(min_energy)\n",
    "        subset = set.pca(min_energy=min_energy)\n",
    "\n",
    "        # Create a list of max likelihood using the traditional cossine similarity and the scaled cossine similarity\n",
    "        max_likelihood_cs = [None]*eval_data_bootstrap.shape[0]\n",
    "        cs_list = [0]*eval_data_bootstrap.shape[0]\n",
    "        \n",
    "        max_likelihood_scs = [None]*eval_data_bootstrap.shape[0]\n",
    "        scs_list = [0]*eval_data_bootstrap.shape[0]\n",
    "\n",
    "        # Classify the eval_data_bootstrap\n",
    "        for subspace in subset:\n",
    "            cs = cossine_similarity(eval_data_bootstrap, subspace)\n",
    "            scs = scaled_cossine_similarity(eval_data_bootstrap, subspace)\n",
    "            for i in range(len(cs)):\n",
    "                if cs[i] > cs_list[i]: cs_list[i] = cs[i]; max_likelihood_cs[i] = subspace.label\n",
    "                if scs[i] > scs_list[i]: scs_list[i] = scs[i]; max_likelihood_scs[i] = subspace.label\n",
    "\n",
    "        correct_class_cs = []\n",
    "        correct_class_scs = []\n",
    "        for l1, l2 in zip(max_likelihood_cs, eval_correct_labels_bootstrap):\n",
    "            correct_class_cs.append(l1 == l2)\n",
    "        for l1, l2 in zip(max_likelihood_scs, eval_correct_labels_bootstrap):\n",
    "            correct_class_scs.append(l1 == l2)\n",
    "\n",
    "        prediction_ratio_cs = correct_class_cs.count(True) / len(correct_class_cs)\n",
    "        prediction_ratio_scs = correct_class_scs.count(True) / len(correct_class_scs)\n",
    "\n",
    "        print(f\"{min_energy:.2f}, cosine_similarity, {prediction_ratio_cs}\")\n",
    "        print(f\"{min_energy:.2f}, scaled_cosine_similarity, {prediction_ratio_scs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
